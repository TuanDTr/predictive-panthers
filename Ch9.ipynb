{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a015f1-6a55-491d-81aa-f2969f84397c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import captum\n",
    "\n",
    "#### gs://marketplace-2xim6sjc/MedMNIST/120/artifactFiles/chestmnist_64.npz\n",
    "\n",
    "# Initialize a client\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and blob (file) names\n",
    "bucket_name = 'marketplace-2xim6sjc'\n",
    "blob_name = 'MedMNIST/120/artifactFiles/chestmnist_64.npz'\n",
    "\n",
    "# Get the bucket and blob\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Download the blob to a local file\n",
    "local_file_path = 'chestmnist_64_local.npz'\n",
    "blob.download_to_filename(local_file_path)\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(local_file_path, allow_pickle=True)\n",
    "\n",
    "# # Access the data\n",
    "# for key in data:\n",
    "#     print(f\"{key}: {data[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7c61b0-79b9-4277-ad6d-887c037f2859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = data[\"train_images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088e4492-1b8c-4d17-826c-094fc0b1b1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv2d' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m preprocess_image(image)\n\u001b[1;32m     76\u001b[0m class_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m243\u001b[39m  \u001b[38;5;66;03m# Example class index\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m gradcam \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_gradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m visualize_gradcam(img_path, gradcam)\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mgenerate_gradcam\u001b[0;34m(img_tensor, class_idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m gradients \u001b[38;5;241m=\u001b[39m gradients\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_layer)\n\u001b[0;32m---> 47\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute the weights\u001b[39;00m\n\u001b[1;32m     50\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(gradients, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2d' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a pretrained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Hook the gradients of the target layer\n",
    "def hook_function(module, grad_in, grad_out):\n",
    "    global gradients\n",
    "    gradients = grad_out[0]\n",
    "\n",
    "# Register hook to the last convolutional layer\n",
    "target_layer = model.layer4[2].conv3\n",
    "target_layer.register_backward_hook(hook_function)\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(img_arr):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = Image.fromarray(img_arr).convert('RGB')\n",
    "    img_tensor = preprocess(img).unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "# Generate Grad-CAM\n",
    "def generate_gradcam(img_tensor, model, layer):\n",
    "    gradcam = captum.attr.GuidedGradCam(model, layer, device_ids=None)\n",
    "   \n",
    "\n",
    "# Visualize the Grad-CAM\n",
    "def visualize_gradcam(img_path, gradcam):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img.resize((224, 224)))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * gradcam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img) / 255\n",
    "    cam = cam / np.max(cam)\n",
    "    plt.imshow(cam)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "img_tensor = preprocess_image(image)\n",
    "class_idx = 243  # Example class index\n",
    "gradcam = generate_gradcam(img_tensor, class_idx)\n",
    "visualize_gradcam(img_path, gradcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02555a91-c915-4280-af82-5d12ed2f7b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740db67-2537-445b-b757-82469b8ad6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
